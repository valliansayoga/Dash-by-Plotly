{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valliansayoga/Dash-by-Plotly/blob/master/EY2025_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ilq71FOeKkgA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h93bybbmwoNl",
        "outputId": "73fcda26-3e4e-4fb6-8c70-ad250c25c8f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'no_data',\n",
              " 1: 'saturated_or_defective_pixel',\n",
              " 2: 'topographic_casted_shadows',\n",
              " 3: 'cloud_shadows',\n",
              " 4: 'vegetation',\n",
              " 5: 'not_vegetated',\n",
              " 6: 'water',\n",
              " 7: 'unclassified',\n",
              " 8: 'cloud_medium_probability',\n",
              " 9: 'cloud_high_probability',\n",
              " 10: 'thin_cirrus',\n",
              " 11: 'snow_or_ice'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "scl_mapping = {\n",
        "    0: 'no_data',\n",
        "    1: 'saturated_or_defective_pixel',\n",
        "    2: 'topographic_casted_shadows',\n",
        "    3: 'cloud_shadows',\n",
        "    4: 'vegetation',\n",
        "    5: 'not_vegetated',\n",
        "    6: 'water',\n",
        "    7: 'unclassified',\n",
        "    8: 'cloud_medium_probability',\n",
        "    9: 'cloud_high_probability',\n",
        "    10: 'thin_cirrus',\n",
        "    11: 'snow_or_ice'\n",
        "}\n",
        "scl_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZXeRJ5xtKweA"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    insample = r2_score(y_train, model.predict(X_train))\n",
        "    outsample = r2_score(y_test, model.predict(X_test))\n",
        "    return insample, outsample\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    insample = r2_score(y_train, model.predict(X_train))\n",
        "    outsample = r2_score(y_test, model.predict(X_test))\n",
        "    return insample, outsample\n",
        "\n",
        "def add_features(df):\n",
        "    df[\"evi_x_lwir\"] = df.evi_median * df.lwir_median\n",
        "    df[\"ndbi_x_lwir\"] = df.ndbi_median * df.lwir_median\n",
        "    df[\"ndbi_/_bldg_dnsty\"] = df.ndbi_median * df.building_density\n",
        "    df[\"pan_chromatic\"] = df.red_median * df.green_median * df.blue_median\n",
        "    df[\"infra_red_combo\"] = df.nir_median * df.swir16_median * df.swir22_median\n",
        "    return df\n",
        "\n",
        "def create_train(df_features, scaler, target=\"UHI Index\", train_size=0.8, indices=None):\n",
        "    print(\"Removing duplicates...\")\n",
        "    rows_before = df_features.shape[0]\n",
        "    check_dupl = df_features.columns[1:]\n",
        "    df_features = df_features.drop_duplicates(subset=check_dupl, keep='first')\n",
        "    rows_after = df_features.shape[0]\n",
        "    print(f\"Removed {rows_before-rows_after} duplicate rows!\")\n",
        "\n",
        "    X = df_features.drop(target, axis=1)\n",
        "    y = df_features[target]\n",
        "\n",
        "    print(\"Scaling...\")\n",
        "    if indices is not None:\n",
        "        X_train, X_test, y_train, y_test = X.iloc[indices[0]], X.iloc[indices[1]], y.iloc[indices[0]], y.iloc[indices[1]]\n",
        "    else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=train_size)\n",
        "\n",
        "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "    print(\"Done\")\n",
        "    return X_train, X_test, y_train, y_test, scaler\n",
        "\n",
        "def load_preprocess_predict_score(models, scaler):\n",
        "    to_drop = [\"Latitude\", \"Longitude\", \"datetime\"]\n",
        "    target = \"UHI Index\"\n",
        "\n",
        "    # Round 1 to get pareto + 1 features\n",
        "    separator = \"-\"*66\n",
        "    spaces = \" \"*24\n",
        "    equals = \"=\"*32\n",
        "\n",
        "    print(spaces, \"Starting round 1\", spaces)\n",
        "    print(separator)\n",
        "    df = pd.read_csv(\"Train_Final.csv\").drop(to_drop, axis=1, errors=\"ignore\")\n",
        "    df = df.pipe(add_features)\n",
        "\n",
        "    X_train, X_test, y_train, y_test, scaler = create_train(\n",
        "        df,\n",
        "        scaler,\n",
        "    )\n",
        "\n",
        "    for model in tqdm(models):\n",
        "        insample, outsample = evaluate_model(model[\"model\"], X_train, X_test, y_train, y_test)\n",
        "        model[\"insample\"] = insample\n",
        "        model[\"outsample\"] = outsample\n",
        "\n",
        "    results = pd.DataFrame(models).sort_values(\"outsample\", ascending=False).reset_index(drop=True)\n",
        "    print(equals, \"Model Scores\", equals)\n",
        "    print(results)\n",
        "    print(separator)\n",
        "    best_model = results.iloc[0]\n",
        "    print(equals, \"Best Model\", equals)\n",
        "    print(best_model.model)\n",
        "    print(separator)\n",
        "\n",
        "    importance = pd.DataFrame(\n",
        "        {\"Features\": X_train.columns, \"Importance\": best_model.model.feature_importances_},\n",
        "    ).sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n",
        "    importance[\"cumulative_importance\"] = importance.Importance.cumsum() / importance.Importance.sum()\n",
        "    print(equals, \"Feature Importance\", equals)\n",
        "    print(importance)\n",
        "    print(separator)\n",
        "\n",
        "    pareto = importance[importance.cumulative_importance <= 0.8]\n",
        "    last_index = pareto.index[-1] + 1\n",
        "    pareto = pd.concat([pareto, importance.iloc[last_index:last_index+1]])\n",
        "    print(equals, \"Pareto Features + 1\", equals)\n",
        "    print(pareto)\n",
        "    print(separator)\n",
        "\n",
        "    print(spaces, \"Starting round 2\", spaces)\n",
        "    print(separator)\n",
        "    df = pd.read_csv(\"Train_Final.csv\").drop(to_drop, axis=1, errors=\"ignore\").pipe(add_features)\n",
        "    use_cols = [target, *pareto.Features]\n",
        "    df = df.loc[:, use_cols]\n",
        "    X_train, X_test, y_train, y_test, scaler = create_train(\n",
        "        df,\n",
        "        scaler,\n",
        "    )\n",
        "\n",
        "    for model in tqdm(models):\n",
        "        insample, outsample = evaluate_model(model[\"model\"], X_train, X_test, y_train, y_test)\n",
        "        model[\"insample\"] = insample\n",
        "        model[\"outsample\"] = outsample\n",
        "\n",
        "    results = pd.DataFrame(models).sort_values(\"outsample\", ascending=False).reset_index(drop=True)\n",
        "    print(equals, \"Model Scores\", equals)\n",
        "    print(results)\n",
        "    print(separator)\n",
        "    best_model = results.iloc[0]\n",
        "    print(equals, \"Best Model\", equals)\n",
        "    print(best_model.model)\n",
        "    print(separator)\n",
        "\n",
        "    importance = pd.DataFrame(\n",
        "        {\"Features\": X_train.columns, \"Importance\": best_model.model.feature_importances_},\n",
        "    ).sort_values(\"Importance\", ascending=False).reset_index(drop=True)\n",
        "    importance[\"cumulative_importance\"] = importance.Importance.cumsum() / importance.Importance.sum()\n",
        "    print(equals, \"Feature Importance\", equals)\n",
        "    print(importance)\n",
        "    print(separator)\n",
        "    return best_model, X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHE-M8VWLSOJ"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "models = [\n",
        "    {\"model\": RandomForestRegressor(random_state=0, n_jobs=-1)},\n",
        "    {\"model\": RandomForestRegressor(250, max_features=0.5, random_state=0, n_jobs=-1)},\n",
        "    {\"model\": RandomForestRegressor(150, max_features=0.5, random_state=0, n_jobs=-1)},\n",
        "    {\"model\": ExtraTreesRegressor(max_features=0.5, n_estimators=200, random_state=0, n_jobs=-1)},\n",
        "    {\"model\": ExtraTreesRegressor(max_features=0.5, n_estimators=250, random_state=0, n_jobs=-1)},\n",
        "    {\"model\": ExtraTreesRegressor(max_features=0.5, n_estimators=100, random_state=0, n_jobs=-1)},\n",
        "]\n",
        "scaler = StandardScaler()\n",
        "best_model, X_train = load_preprocess_predict_score(models, scaler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwE-SpCdy-KV",
        "outputId": "1db356c6-eac5-4abd-bb17-c4104fb958a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Starting round 1                         \n",
            "------------------------------------------------------------------\n",
            "Removing duplicates...\n",
            "Removed 0 duplicate rows!\n",
            "Scaling...\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [01:38<00:00, 16.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================ Model Scores ================================\n",
            "                                               model  insample  outsample\n",
            "0  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.959008\n",
            "1  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.958958\n",
            "2  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.958325\n",
            "3  (DecisionTreeRegressor(max_features=0.5, rando...  0.992495   0.946768\n",
            "4  (DecisionTreeRegressor(max_features=0.5, rando...  0.992568   0.946714\n",
            "5  (DecisionTreeRegressor(max_features=1.0, rando...  0.992141   0.944762\n",
            "------------------------------------------------------------------\n",
            "================================ Best Model ================================\n",
            "ExtraTreesRegressor(max_features=0.5, n_estimators=250, n_jobs=-1,\n",
            "                    random_state=0)\n",
            "------------------------------------------------------------------\n",
            "================================ Feature Importance ================================\n",
            "                     Features  Importance  cumulative_importance\n",
            "0              distance_range    0.122817               0.122817\n",
            "1                std_distance    0.118752               0.241569\n",
            "2          distance_variation    0.115638               0.357207\n",
            "3            average_distance    0.108058               0.465265\n",
            "4                 lwir_median    0.034658               0.499923\n",
            "5                  wvp_median    0.028647               0.528570\n",
            "6                        emsd    0.028055               0.556625\n",
            "7        coast_aerosol_median    0.025116               0.581741\n",
            "8                 ndwi_median    0.023693               0.605434\n",
            "9                 ndvi_median    0.022529               0.627963\n",
            "10                 nir_median    0.019576               0.647540\n",
            "11                 evi_median    0.019516               0.667055\n",
            "12                 evi_x_lwir    0.019233               0.686288\n",
            "13   neighboring_intersection    0.018909               0.705197\n",
            "14      nearest_building_size    0.018631               0.723828\n",
            "15     avg_polygon_complexity    0.018575               0.742403\n",
            "16              swir22_median    0.017852               0.760255\n",
            "17               green_median    0.017368               0.777622\n",
            "18                ndmi_median    0.016548               0.794171\n",
            "19      building_area_density    0.016529               0.810700\n",
            "20                ndbi_x_lwir    0.016446               0.827145\n",
            "21              swir16_median    0.015632               0.842778\n",
            "22                 red_median    0.015483               0.858261\n",
            "23                ndbi_median    0.015156               0.873417\n",
            "24                blue_median    0.014804               0.888222\n",
            "25            infra_red_combo    0.013851               0.902073\n",
            "26              pan_chromatic    0.013724               0.915797\n",
            "27  nearest_building_distance    0.012641               0.928439\n",
            "28      nearest_polygon_angle    0.010488               0.938926\n",
            "29          ndbi_/_bldg_dnsty    0.009393               0.948319\n",
            "30           building_density    0.008619               0.956938\n",
            "31  50m_nearby_building_count    0.008416               0.965354\n",
            "32  40m_nearby_building_count    0.007231               0.972585\n",
            "33  10m_nearby_building_count    0.006684               0.979269\n",
            "34  30m_nearby_building_count    0.006434               0.985703\n",
            "35  20m_nearby_building_count    0.006387               0.992090\n",
            "36                 scl_median    0.003852               0.995941\n",
            "37              is_a_building    0.003383               0.999325\n",
            "38                 qa_aerosol    0.000628               0.999953\n",
            "39          relative_position    0.000047               1.000000\n",
            "40                 aot_median    0.000000               1.000000\n",
            "------------------------------------------------------------------\n",
            "================================ Pareto Features + 1 ================================\n",
            "                    Features  Importance  cumulative_importance\n",
            "0             distance_range    0.122817               0.122817\n",
            "1               std_distance    0.118752               0.241569\n",
            "2         distance_variation    0.115638               0.357207\n",
            "3           average_distance    0.108058               0.465265\n",
            "4                lwir_median    0.034658               0.499923\n",
            "5                 wvp_median    0.028647               0.528570\n",
            "6                       emsd    0.028055               0.556625\n",
            "7       coast_aerosol_median    0.025116               0.581741\n",
            "8                ndwi_median    0.023693               0.605434\n",
            "9                ndvi_median    0.022529               0.627963\n",
            "10                nir_median    0.019576               0.647540\n",
            "11                evi_median    0.019516               0.667055\n",
            "12                evi_x_lwir    0.019233               0.686288\n",
            "13  neighboring_intersection    0.018909               0.705197\n",
            "14     nearest_building_size    0.018631               0.723828\n",
            "15    avg_polygon_complexity    0.018575               0.742403\n",
            "16             swir22_median    0.017852               0.760255\n",
            "17              green_median    0.017368               0.777622\n",
            "18               ndmi_median    0.016548               0.794171\n",
            "19     building_area_density    0.016529               0.810700\n",
            "------------------------------------------------------------------\n",
            "                         Starting round 2                         \n",
            "------------------------------------------------------------------\n",
            "Removing duplicates...\n",
            "Removed 0 duplicate rows!\n",
            "Scaling...\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:59<00:00,  9.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================ Model Scores ================================\n",
            "                                               model  insample  outsample\n",
            "0  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.966532\n",
            "1  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.966305\n",
            "2  (ExtraTreeRegressor(max_features=0.5, random_s...  1.000000   0.966272\n",
            "3  (DecisionTreeRegressor(max_features=0.5, rando...  0.993428   0.953088\n",
            "4  (DecisionTreeRegressor(max_features=0.5, rando...  0.993287   0.952487\n",
            "5  (DecisionTreeRegressor(max_features=1.0, rando...  0.993052   0.951557\n",
            "------------------------------------------------------------------\n",
            "================================ Best Model ================================\n",
            "ExtraTreesRegressor(max_features=0.5, n_estimators=200, n_jobs=-1,\n",
            "                    random_state=0)\n",
            "------------------------------------------------------------------\n",
            "================================ Feature Importance ================================\n",
            "                    Features  Importance  cumulative_importance\n",
            "0               std_distance    0.135573               0.135573\n",
            "1             distance_range    0.122472               0.258046\n",
            "2           average_distance    0.121731               0.379777\n",
            "3         distance_variation    0.121077               0.500854\n",
            "4                lwir_median    0.045554               0.546408\n",
            "5                 wvp_median    0.038619               0.585027\n",
            "6                       emsd    0.037849               0.622876\n",
            "7       coast_aerosol_median    0.037742               0.660618\n",
            "8                ndvi_median    0.030558               0.691175\n",
            "9                ndmi_median    0.030367               0.721542\n",
            "10             swir22_median    0.029905               0.751448\n",
            "11               ndwi_median    0.029403               0.780851\n",
            "12                evi_median    0.029138               0.809989\n",
            "13              green_median    0.029051               0.839039\n",
            "14                nir_median    0.028690               0.867729\n",
            "15     nearest_building_size    0.027506               0.895235\n",
            "16     building_area_density    0.026934               0.922169\n",
            "17  neighboring_intersection    0.026742               0.948911\n",
            "18    avg_polygon_complexity    0.025601               0.974511\n",
            "19                evi_x_lwir    0.025489               1.000000\n",
            "------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5wrTpjrOTjw"
      },
      "source": [
        "# Predicting Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMk4aXVpOVdm",
        "outputId": "0667c78f-0d91-4c14-d716-629aa7f94cf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting 1040 rows...\n",
            "Predicting...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def create_submission(filename: str, model, scaler):\n",
        "    sub_df = pd.read_csv(\"Submission_Final.csv\")\n",
        "    final_df = sub_df[[\"Latitude\", \"Longitude\"]].copy()\n",
        "    print(\"Predicting\", sub_df.shape[0], \"rows...\")\n",
        "\n",
        "    ############################\n",
        "    sub_df = add_features(sub_df)\n",
        "\n",
        "    # # # # # Comment if not used!\n",
        "    # sub_df.scl_median = sub_df.scl_median.map(scl_mapping)\n",
        "    # scl_ohe = ohe.transform(sub_df.loc[:, [\"scl_median\"]])\n",
        "    # scl_ohe = pd.DataFrame(scl_ohe, columns=ohe.get_feature_names_out([\"scl_median\"]))\n",
        "    # sub_df = pd.concat([sub_df.drop(\"scl_median\", axis=1), scl_ohe], axis=1)\n",
        "\n",
        "    to_predict = pd.DataFrame(\n",
        "        scaler.transform(sub_df.loc[:, X_train.columns]),\n",
        "        columns=X_train.columns\n",
        "    )\n",
        "\n",
        "    print(\"Predicting...\")\n",
        "    final_df[\"UHI Index\"] = model.predict(to_predict)\n",
        "    final_df.to_csv(filename, index=False)\n",
        "    print(\"Done!\")\n",
        "    return\n",
        "create_submission(\"BestModel_Radius100_Pareto_Engineered.csv\", best_model.model, scaler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KF1LwRc4dyx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsHYz8_ZERoB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpyuciUodXdbNA1+ZTz6jq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}